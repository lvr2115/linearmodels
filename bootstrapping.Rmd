---
title: "Bootstrapping"
author: "Laura Robles-Torres"
date: "2023-11-30"
output: github_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)

set.seed(1)

```

Bootstrapping is a popular resampling-based approach to statistical inference, and is helpful when usual statistical methods are intractable or inappropriate. The idea is to draw repeated samples from your original sample with replacement, thereby approximating the repeated sampling framework. Using list columns to store bootstrap samples is natural and provides a “tidy” approach to resampling-based inference.

--------------------------------------------------------

# Repeated sampling

## Why?
Repeated sampling is a conceptual framework that underlies most of statistics. If you want to know something about true pop. mean, you will draw a sample and compute a sample mean. If you understand what would happen with MULTIPLE same size samples from a population, you can say something about the distribution of your mean in your sample. The sample mean and dist. of it can be used to create a CI and make statements on what approximates the truth. The distribution of the sample mean converges to a normal distribution.

Traditionally, the distribution of a sample statistic (sample mean, SLR coefficients, etc.) for repeated, random draws from a population has been established theoretically. These theoretical distributions make some assumptions about the underlying population from which samples are drawn, or depend on large sample sizes for asymptotic results. Normal distribution is known/assumed in those settings. 

In cases where the *assumptions aren’t met*, or *sample sizes aren’t large enough* for asymptotics to kick in, it is still necessary to make inferences using the sample statistic. In these cases, drawing repeatedly from the original population would be great – one could simple draw a lot of samples and look at the empirical (rather than theoretical) distribution. But, as we said in iteration and simulation, repeated sampling just doesn’t happen in the real world.

I don't know what distribution this follows but Im gonna repeat a sample a bunch of times and look at the distribution of what I'm interested to see what the confidence intervals, range, distribution of my data is. 

## What is it?

The idea is to mimic repeated sampling with the one sample you have. Your sample is drawn at random from your population. 
A *bootstrap sample* is drawn from the one sample you do have. It has:
   - the same size as the original sample
   - is drawn with replacement. (You draw a person ID 4 into your    bootstrap sample, and it can then go back into the original        population and could be drawn again)
   
You analyze this sample using whatever approach you want to apply and repeat. 

## How?

Coding the bootstrap uses a function to:
 - draw a sample with replacement
 - analyze the sample
 - return object of interest
and repeat this many times.

We use list columns to keep track of the bootstrap samples, analyses and results in a single df.

--------------------------------------------------------

## Simulate data:

First I’ll generate 'x', then an 'error' sampled from a normal distribution, and then a response 'y'; this all gets stored in dataframe 'sim_df_const'. Then I’ll modify this by multiplying the errors by a term that involves x, and create a new response variable y.

```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )


#this second df has the same x values n takes error terms and multiplies them and generates y.

sim_df_nonconst = 
  sim_df_const |> 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)
```

Plot to see it. Residuals have constant variance across x axis in this linear regression (first plot). In second plot, when x is close to 0, the error is pretty small. but when x is far from zero, the residuals spread out more further from the line. we would not be able to meet the assumption that the variance of the residuals is constant in a linear regression so we wouldnt be able to do the lm model as we do. you could still fit and get estimates, but the problem comes from trying to guess confidence intervals and uncertainty in your model. 

```{r}
sim_df_const |>
  ggplot(aes(x=x, y=y))+geom_point()+geom_smooth(method="lm")

sim_df_nonconst |>
  ggplot(aes(x=x, y=y))+geom_point()+geom_smooth(method="lm")

```

```{r}
lm(y~x, data=sim_df_const) |> broom::tidy()
lm(y~x, data=sim_df_nonconst) |> broom::tidy()
```

You can get estimates and CI based on usual assumptions and you can get results as if those assumptions are true. We don't really see that standard error is higher for the second dataset, even though we should. Despite the very different error structures, standard errors for coefficient estimates are similar in both cases!

We’ll use the bootstrap to make inference for the data on the right. This is intended largely as an illustration for how to use the bootstrap in cases where the theoretical distribution is “unknown”, although for these data in particular weighted least squares could be more appropriate.

I know assumptions aren't met here...I need to understand what the dist. of my slope and intercept are under repeated sampling using bootstrap.

# Drawing one bootstrap sample

### Function to generate bootstrap samples
- Argument is the dataframe
- Return object is a sample from that df drawn with replacement
```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE) |>
    arrange(x)
}

#sample_frac draws a sample of a particular proportion of your dataset. we want it to be the same size as the df, so we saw "(df" and we say replace=true so that its with replacement.  
```

### Check if it works

```{r}
#applies function to dataset
boot_sample(sim_df_nonconst)

boot_sample(sim_df_nonconst) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .3) +
  stat_smooth(method = "lm")
```

(Darker obs means the same obs is seen multiple times)

That looks about right. In comparison with the original data, the bootstrap sample has the same characteristics but isn’t a perfect duplicate – some original data points appear more than once, others don’t appear at all.

# Drawing many bootstrap samples

This will create a list column to keep track of every repeated sample with replacement. 

```{r}
boot_straps = 
  tibble(
    strap_number = 1:1000, 
    strap_sample = rerun(1000, boot_sample(df = sim_df_nonconst))
  )

boot_straps
```

Checking if it worked by looking at a couple 
```{r}
boot_straps |> 
  slice(1:3) |> 
  mutate(strap_sample = map(strap_sample, arrange, x)) |> 
  pull(strap_sample)

boot_straps |> 
  slice(1:3) |> 
  unnest(strap_sample) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm", se = FALSE) +
  facet_grid(~strap_number) 
```
This shows some of the differences across bootstrap samples, and shows that the fitted regression lines aren’t the same for every bootstrap sample.


# Analyzing these samples 

To do that, I’ll use the analytic pipeline we established when looking at nested datasets in linear models: fit the regression model; tidy the output; unnest and examine the results. The code chunk below uses this pipeline to look at bootstrap standard errors for the estimated regression coefficients.

```{r}
bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(.x=strap_sample,~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)) |> 
  select(strap_number, results) |> 
  unnest(results) 

bootstrap_results |> 
  group_by(term) |> 
  summarize(
    mean_est=mean(estimate),
    sd_est = sd(estimate)) |> 
  knitr::kable(digits = 3)
```

What do I have now?
  
A long dataframe with 2,000 rows. I have 1,000 datasets and I got an intercept and slope for each one of them. 

Comparing these to the results of ordinary least squares, the standard error for the intercept is much smaller and the standard error for the intercept is a bit larger. This is reasonable, given the non-constant variance in the data given smaller residuals around zero and larger residuals in the the tails of the x distribution.

Looking at the distribution. This is the kind of distribution we would see if we did repeated sampling of this population. 

```{r}
bootstrap_results |>
  filter(term=="x")|>
  ggplot(aes(x=estimate))+
  geom_density()
```

## Confidence Intervals 

For a 95% CI, we might try to exclude the lower and upper 2.5% of the distribution of parameter estimates across “repeated” samples. The code below will do that.
```{r}
bootstrap_results |> 
  group_by(term) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975))
```

For a simple linear regression, we can show the fitted lines for each bootstrap sample to build intuition for these results.
In comparison to the standard error bands in our previous plot (which are based on OLS), the distribution of regression lines is narrower near x=0 and wider at the ends of the x distribution.


```{r}
boot_straps |> 
  unnest(strap_sample) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_line(aes(group = strap_number), stat = "smooth", method = "lm", se = FALSE, alpha = .1, color = "blue") +
  geom_point(data = sim_df_nonconst, alpha = .5)
```


# Bootstrap using modelr
Bootstrapping is common enough that it’s been automated, to some degree, in the modelr::boostrap function. Bootstrapping is common enough that it’s been automated, to some degree, in the modelr::boostrap function. Doing the function/sampling, analyzing, and interpreting in one chunk...:

```{r}
boot_straps = 
  sim_df_nonconst |> 
  modelr::bootstrap(n = 1000, id="strap_number") |>
  mutate(
    models = map(.x=strap,~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) |> 
  select(strap_number, results) |> 
  unnest(results) |>
  group_by(term) |> 
  summarize(
    mean_est=mean(estimate),
    sd_est = sd(estimate))

```

## Revisiting Airbnb data

```{r import data}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb |> 
  mutate(stars = review_scores_location / 2) |> 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) |> 
  filter(borough != "Staten Island") |> 
  select(price, stars, borough, neighborhood, room_type)
```

```{r plot}
nyc_airbnb |> 
  ggplot(aes(x = stars, y = price)) + geom_point()
```

In this plot (and in linear models, we noticed that some large outliers in price might affect estimates and inference for the association between star rating and price. Because estimates are likely to be sensitive to those outliers and “usual” rules for inference may not apply, the code chunk below uses the bootstrap to examine the distribution of regression coefficients under repeated sampling.

```{r}
nyc_airbnb |> 
  filter(borough == "Manhattan") |> 
  drop_na(stars) |>
  modelr::bootstrap(n = 1000, id="strap_number") |> 
  mutate(
    models = map(.x=strap,~lm(price ~ stars, data = .x)),
    results = map(models, broom::tidy)
  ) |> 
  select(strap_number, results) |> 
  unnest(results) |>
  group_by(term) |> 
  summarize(
    mean_est=mean(estimate),
    sd_est = sd(estimate))
```

